{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compressed-ancient",
   "metadata": {},
   "source": [
    "## Home loan approval with LSTM in PyTorch\n",
    "\n",
    "https://www.kaggle.com/datasets/rishikeshkonapure/home-loan-approval?select=loan_sanction_train.csv - link to .csv file to use\n",
    "\n",
    "\n",
    "\n",
    "Mudar o nÂº de inputs e adicionar mais camadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the torch library and the torch.nn library.\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The `TimeSeriesModel` class inherits from the `nn.Module` class, and it has two layers: an LSTM\n",
    "# layer and a linear layer\n",
    "class TimeSeriesModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        The TimeSeriesModel class inherits from the nn.Module class, and it has two sub-modules: lstm and\n",
    "        linear\n",
    "        \n",
    "        :param input_size: The number of features in the input data\n",
    "        :param hidden_size: The number of features in the hidden state h\n",
    "        :param output_size: The number of features in the output\n",
    "        \"\"\"\n",
    "        # Calling the `__init__` method of the `nn.Module` class.\n",
    "        super(TimeSeriesModel, self).__init__()\n",
    "        # Creating an LSTM layer with input size 1 and hidden size 10.\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        # Creating a linear layer with input size 10 and output size 1.\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        The forward function takes in a tensor x, passes it through an LSTM layer (lstm), and finally\n",
    "        through a linear layer (linear). It will also return the output of the linear layer\n",
    "        \n",
    "        :param x: input data\n",
    "        :return: The last output of the LSTM layer and the hidden state of the LSTM layer.\n",
    "        \"\"\"\n",
    "       # The LSTM layer returns two outputs: the output of the LSTM layer and the hidden state of the\n",
    "       # LSTM layer.\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # The LSTM layer returns two outputs: the output of the LSTM layer and the hidden state of the\n",
    "        # LSTM layer. The output of the LSTM layer is a tensor with shape (seq_len, batch_size,\n",
    "        # num_features). The hidden state of the LSTM layer is a tensor with shape (num_layers,\n",
    "        # batch_size, num_features). In this case, we only care about the last output of the LSTM\n",
    "        # layer, which is the output of the LSTM layer at the last time step.\n",
    "        pred = self.linear(lstm_out[-1])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-brunswick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_size = 1\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "model = TimeSeriesModel(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# Creating an Adam optimizer with the parameters of the model.\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        # Setting the gradients of all the parameters of the model to zero.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Calculating the gradients of the loss with respect to the parameters of the model.\n",
    "        loss.backward()\n",
    "        # It updates the parameters of the model.\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envi",
   "language": "python",
   "name": "envi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
